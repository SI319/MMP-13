{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSFbIMb87cHu"
      },
      "source": [
        "# **Bioinformatics Project**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iQiERxumDor"
      },
      "source": [
        "## **ChEMBL Database**\n",
        "\n",
        "The [*ChEMBL Database*](https://www.ebi.ac.uk/chembl/) is a database that contains curated bioactivity data of more than 2.5 million compounds. It is compiled from more than 92,100 documents, 1.7 million assays and the data spans 16,000 targets and 2,100 cells and 48,800 indications.\n",
        "[Data of May 07, 2025]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryGAwAIQ4yf"
      },
      "source": [
        "## **Installing libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toGT1U_B7F2i"
      },
      "source": [
        "Install the ChEMBL web service package so that we can retrieve bioactivity data from the ChEMBL Database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJGExHQBfLh7"
      },
      "outputs": [],
      "source": [
        "! pip install chembl_webresource_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0kJjL8gb5nX"
      },
      "source": [
        "## **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXoCvMPPfNrv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from chembl_webresource_client.new_client import new_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lBsDrD0gAqH"
      },
      "source": [
        "### **Target search for MMP-13**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxtp79so4ZjF"
      },
      "outputs": [],
      "source": [
        "# Target search for MMP-13\n",
        "target = new_client.target\n",
        "target_query = target.search('CHEMBL280')\n",
        "targets = pd.DataFrame.from_dict(target_query)\n",
        "targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5OPfEALjAfZ"
      },
      "source": [
        ":### **Select and retrieve bioactivity data for *MMP13* (First Entry)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSQ3aroOgML7"
      },
      "source": [
        "We will assign the 1st entry (which corresponds to the target protein, *MMP13*) to the ***selected_target*** variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StrcHMVLha7u"
      },
      "outputs": [],
      "source": [
        "selected_target = targets.target_chembl_id[0]\n",
        "selected_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWd2DRalgjzB"
      },
      "source": [
        "Here, we will retrieve only bioactivity data for *MMP13* (CHEMBL280) that are reported as pChEMBL values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeFbV_CsSP8D"
      },
      "outputs": [],
      "source": [
        "activity = new_client.activity\n",
        "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrzAcEFnYzBY"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9iUAXFdSkoM"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXMpFNUOn_8"
      },
      "source": [
        "## **Handling missing data**\n",
        "If any compounds has missing value for the **standard_value** and **canonical_smiles** column then drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVOdk6ZR396"
      },
      "outputs": [],
      "source": [
        "df2 = df[df.standard_value.notna()]\n",
        "df2 = df2[df.canonical_smiles.notna()]\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCkuaeVNbdtM"
      },
      "outputs": [],
      "source": [
        "len(df2.canonical_smiles.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbAK3K7wclkS"
      },
      "outputs": [],
      "source": [
        "df2_nr = df2.drop_duplicates(['canonical_smiles'])\n",
        "df2_nr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H4sSFAWhV9B"
      },
      "source": [
        "## **Data pre-processing of the bioactivity data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv2dzid_hzKd"
      },
      "source": [
        "### **Combine the 3 columns (molecule_chembl_id,canonical_smiles,standard_value) and bioactivity_class into a DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NCLYmrASgha"
      },
      "outputs": [],
      "source": [
        "selection = ['molecule_chembl_id','canonical_smiles','standard_value']\n",
        "df3 = df2_nr[selection]\n",
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ObVWGmkj_L"
      },
      "source": [
        "Saves dataframe to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAzJTVu4kMd_"
      },
      "outputs": [],
      "source": [
        "df3.to_csv('mmp13-2665.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skuSjuDu3Vrm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('mmp13-2665.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO22XVlzhkXR"
      },
      "source": [
        "### **Labeling compounds as either being active, inactive or intermediate**\n",
        "The bioactivity data is in the IC50 unit. Compounds having values of less than 1nM and 100 nM will be considered to be **active** while those greater than 10,00 nM will be considered to be **inactive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrbgNRGYFDpG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck9Z2M9CkRy_"
      },
      "outputs": [],
      "source": [
        "df4 = pd.read_csv('mmp13-2665.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnfKPbctEmSA"
      },
      "outputs": [],
      "source": [
        "df4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bioactivity_threshold = []\n",
        "for i in df4.standard_value:\n",
        "  if float(i) >= 100:\n",
        "    bioactivity_threshold.append(\"inactive\")\n",
        "  else: float(i) <= 100:\n",
        "    bioactivity_threshold.append(\"active\")"
      ],
      "metadata": {
        "id": "rPKqSz7tMbcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li64nUiZQ-y2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "bioactivity_class = pd.Series(bioactivity_threshold, name='class')\n",
        "df5 = pd.concat([df4, bioactivity_class], axis=1)\n",
        "df5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tlgyexWh7YJ"
      },
      "source": [
        "Saves dataframe to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSNia7suXstR"
      },
      "outputs": [],
      "source": [
        "df5.to_csv('mmp13.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJKZ9uKhsEdP"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('mmp13.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Y7_lgN4jzM"
      },
      "source": [
        "# **Exploratory Data Analysis**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-4IOizard4P"
      },
      "source": [
        "## **Install conda and rdkit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0mjQ2PcrSe5"
      },
      "outputs": [],
      "source": [
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "! conda install -c rdkit rdkit -y\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmxXXFa4wTNG"
      },
      "source": [
        "## **Load bioactivity data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AMm19NW0bJR"
      },
      "outputs": [],
      "source": [
        "df5_no_smiles = df5.drop(columns='canonical_smiles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpkitQUutGAF"
      },
      "outputs": [],
      "source": [
        "df5_no_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aymiQsfdr5sY"
      },
      "outputs": [],
      "source": [
        "smiles = []\n",
        "\n",
        "for i in df5.canonical_smiles.tolist():\n",
        "  cpd = str(i).split('.')\n",
        "  cpd_longest = max(cpd, key = len)\n",
        "  smiles.append(cpd_longest)\n",
        "\n",
        "smiles = pd.Series(smiles, name = 'canonical_smiles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7a8qW_U85ZK"
      },
      "outputs": [],
      "source": [
        "df5_clean_smiles = pd.concat([df5_no_smiles,smiles], axis=1)\n",
        "df5_clean_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4xAp-NBtQtS"
      },
      "outputs": [],
      "source": [
        "df1 = df5_clean_smiles.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6Bf0uOGtef6"
      },
      "outputs": [],
      "source": [
        "df1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzN_S4Quro5S"
      },
      "source": [
        "## **Calculate Lipinski descriptors**\n",
        "Christopher Lipinski, a scientist at Pfizer, came up with a set of rule-of-thumb for evaluating the **druglikeness** of compounds. Such druglikeness is based on the Absorption, Distribution, Metabolism and Excretion (ADME) that is also known as the pharmacokinetic profile. Lipinski analyzed all orally active FDA-approved drugs in the formulation of what is to be known as the **Rule-of-Five** or **Lipinski's Rule**.\n",
        "\n",
        "The Lipinski's Rule stated the following:\n",
        "* Molecular weight < 500 Dalton\n",
        "* Octanol-water partition coefficient (LogP) < 5\n",
        "* Hydrogen bond donors < 5\n",
        "* Hydrogen bond acceptors < 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qn_eQcnxY7C"
      },
      "source": [
        "### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kibIqztBFTVo"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit-pypi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgBjIdT-rnRU"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem import Descriptors, Lipinski\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsgTV-ByxdMa"
      },
      "source": [
        "### **Calculate descriptors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCXEY7a9ugO_"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def lipinski(smiles, verbose=False):\n",
        "    moldata = []\n",
        "    for elem in smiles:\n",
        "        mol = Chem.MolFromSmiles(elem)\n",
        "        moldata.append(mol)\n",
        "\n",
        "    baseData = np.arange(1, 1)\n",
        "    i = 0\n",
        "    for mol in moldata:\n",
        "        desc_MolWt = Descriptors.MolWt(mol)\n",
        "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
        "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
        "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
        "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
        "\n",
        "\n",
        "        # Calculate TPSA\n",
        "        desc_TPSA = Descriptors.TPSA(mol)\n",
        "\n",
        "        row = np.array([desc_MolWt,\n",
        "                        desc_MolLogP,\n",
        "                        desc_NumHDonors,\n",
        "                        desc_NumHAcceptors,\n",
        "                        desc_NumRotatableBonds,\n",
        "                        desc_TPSA  # Add TPSA to the array\n",
        "                        ])\n",
        "\n",
        "        if i == 0:\n",
        "            baseData = row\n",
        "        else:\n",
        "            baseData = np.vstack([baseData, row])\n",
        "        i += 1\n",
        "\n",
        "    columnNames = [\"MW\", \"LogP\", \"NumHDonors\", \"NumHAcceptors\", \"NumRotatableBonds\", \"TPSA\"]\n",
        "    descriptors = pd.DataFrame(data=baseData, columns=columnNames)\n",
        "\n",
        "    return descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThFIFw8IukMY"
      },
      "outputs": [],
      "source": [
        "df_lipinski = lipinski(df1.canonical_smiles)\n",
        "df_lipinski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMlPfFrxicj"
      },
      "source": [
        "### **Combine DataFrames**\n",
        "\n",
        "Let's take a look at the 2 DataFrames that will be combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaezyM5vwp9n"
      },
      "outputs": [],
      "source": [
        "df_lipinski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTs232RcupOu"
      },
      "outputs": [],
      "source": [
        "df6 = df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eET6iZ1Aw3oe"
      },
      "source": [
        "Now, let's combine the 2 DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9nUZC0Ww3gp"
      },
      "outputs": [],
      "source": [
        "df_combined = pd.concat([df6,df_lipinski], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRBfBP3QxFJp"
      },
      "outputs": [],
      "source": [
        "df_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sugHc8AjSiCW"
      },
      "outputs": [],
      "source": [
        "df_combined.to_csv('Lipinski.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI0lH8A8wVIl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('Lipinski.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0MLOedB6j96"
      },
      "source": [
        "### **Convert IC50 to pIC50**\n",
        "To allow **IC50** data to be more uniformly distributed, we will convert **IC50** to the negative logarithmic scale which is essentially **-log10(IC50)**.\n",
        "\n",
        "This custom function pIC50() will accept a DataFrame as input and will:\n",
        "* Take the IC50 values from the ``standard_value`` column and converts it from nM to M by multiplying the value by 10$^{-9}$\n",
        "* Take the molar value and apply -log10\n",
        "* Delete the ``standard_value`` column and create a new ``pIC50`` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXMuFQoQ4pZF"
      },
      "outputs": [],
      "source": [
        "# https://github.com/chaninlab/estrogen-receptor-alpha-qsar/blob/master/02_ER_alpha_RO5.ipynb\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pIC50(input):\n",
        "    pIC50 = []\n",
        "\n",
        "    for i in input['standard_value_norm']:\n",
        "        molar = i*(10**-9) # Converts nM to M\n",
        "        pIC50.append(-np.log10(molar))\n",
        "\n",
        "    input['pIC50'] = pIC50\n",
        "    x = input.drop('standard_value_norm', axis=1)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU5Fh1h2OaJJ"
      },
      "source": [
        "Point to note: Values greater than 100,000,000 will be fixed at 100,000,000 otherwise the negative logarithmic value will become negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuUTFUpcR1wU"
      },
      "outputs": [],
      "source": [
        "df_combined.standard_value.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyiJ0to5N6Z_"
      },
      "outputs": [],
      "source": [
        "-np.log10( (10**-9)* 100000000 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S1aJkOYOP6K"
      },
      "outputs": [],
      "source": [
        "-np.log10( (10**-9)* 10000000000 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkrTs7RfPsrH"
      },
      "source": [
        "We will first apply the norm_value() function so that the values in the standard_value column is normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A81u0MSZtkhZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def norm_value(input):\n",
        "    norm = []\n",
        "\n",
        "    for i in input['standard_value']:\n",
        "        if i > 100000000:\n",
        "          i = 100000000\n",
        "        norm.append(i)\n",
        "\n",
        "    input['standard_value_norm'] = norm\n",
        "    x = input.drop('standard_value', axis=1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeSy_NVAsJr0"
      },
      "outputs": [],
      "source": [
        "df_norm = norm_value(df_combined)\n",
        "df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeCd9LPGI0vh"
      },
      "outputs": [],
      "source": [
        "df_norm.standard_value_norm.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX2Mj2-ZP1Rj"
      },
      "outputs": [],
      "source": [
        "df_final = pIC50(df_norm)\n",
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8lPVlz8ISPj"
      },
      "outputs": [],
      "source": [
        "df_final.to_csv('MMP13_pIC50.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY3Il4DHIb9a"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('MMP13_pIC50.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDKZzmK57YnS"
      },
      "outputs": [],
      "source": [
        "df_final.pIC50.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0vqbQWfxsZu"
      },
      "source": [
        "## **Exploratory Data Analysis (Chemical Space Analysis) via Lipinski descriptors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18heJagiyHoF"
      },
      "source": [
        "### **Import library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAvPM2K1JmIY"
      },
      "outputs": [],
      "source": [
        "df_final=pd.read_csv('MMP13_pIC50_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "metadata": {
        "id": "ugfYfvg5R4MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Egq_rNsxtIj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set(style='ticks')\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiarmFbOdG3H"
      },
      "source": [
        "### **Frequency plot of the 2 bioactivity classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-E_XQIwGffK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6.5, 6.5))\n",
        "order = ['active','inactive']\n",
        "\n",
        "# Set the order of the bars\n",
        "\n",
        "sns.countplot(x='bioactivity_class', data=df_final, edgecolor='black', order=order, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('bioactivity_class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_bioactivity_class.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB68NKVG0j68"
      },
      "source": [
        "### **Scatter plot of MW versus LogP**\n",
        "\n",
        "It can be seen that the 2 bioactivity classes are spanning similar chemical spaces as evident by the scatter plot of MW vs LogP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT6Y9kEuLj3G"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.scatterplot(x='MW', y='LogP', data=df_final, hue='bioactivity_class', size='pIC50', edgecolor='black', alpha=0.5)\n",
        "\n",
        "plt.xlabel('MW', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('LogP', fontsize=14, fontweight='bold')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
        "plt.savefig('plot_MW_vs_LogP.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLAfyRwHyJfX"
      },
      "source": [
        "### **Box plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n1uIAivyOkY"
      },
      "source": [
        "#### **pIC50 value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDnMNIDKKyJy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'pIC50', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('pIC50 value', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_ic50.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsOqKyysCZCv"
      },
      "source": [
        "**Statistical analysis | Mann-Whitney U Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKasIJ-xK_ge"
      },
      "outputs": [],
      "source": [
        "def mannwhitney(descriptor, verbose=False):\n",
        "  # https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/\n",
        "  from numpy.random import seed\n",
        "  from numpy.random import randn\n",
        "  from scipy.stats import mannwhitneyu\n",
        "\n",
        "# seed the random number generator\n",
        "  seed(1)\n",
        "\n",
        "# actives and inactives\n",
        "  selection = [descriptor, 'bioactivity_class']\n",
        "  df = df_final[selection]\n",
        "  active = df[df.bioactivity_class == 'active']\n",
        "  active = active[descriptor]\n",
        "\n",
        "  selection = [descriptor, 'bioactivity_class']\n",
        "  df = df_final[selection]\n",
        "  inactive = df[df.bioactivity_class == 'inactive']\n",
        "  inactive = inactive[descriptor]\n",
        "\n",
        "# compare samples\n",
        "  stat, p = mannwhitneyu(active, inactive)\n",
        "  #print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "# interpret\n",
        "  alpha = 0.05\n",
        "  if p > alpha:\n",
        "    interpretation = 'Same distribution (fail to reject H0)'\n",
        "  else:\n",
        "    interpretation = 'Different distribution (reject H0)'\n",
        "\n",
        "  results = pd.DataFrame({'Descriptor':descriptor,\n",
        "                          'Statistics':stat,\n",
        "                          'p':p,\n",
        "                          'alpha':alpha,\n",
        "                          'Interpretation':interpretation}, index=[0])\n",
        "  filename = 'mannwhitneyu_' + descriptor + '.csv'\n",
        "  results.to_csv(filename)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZmUgOmdYVm5"
      },
      "outputs": [],
      "source": [
        "mannwhitney('pIC50')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2UlCwPmyTBq"
      },
      "source": [
        "#### **MW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNlEEsDEx3m6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'MW', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('MW Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('MW', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_MW.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRl2FvgHYqaG"
      },
      "outputs": [],
      "source": [
        "mannwhitney('MW')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5hyBhGqyc6J"
      },
      "source": [
        "#### **LogP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liEtkpI4yX9t"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'LogP', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('LogP Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('LogP', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_LogP.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KgV5v_oFLXh"
      },
      "source": [
        "**Statistical analysis | Mann-Whitney U Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B61UsGMIFLuE"
      },
      "outputs": [],
      "source": [
        "mannwhitney('LogP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4db7LZLRym2k"
      },
      "source": [
        "#### **NumHDonors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iru1JPM1yg5A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'NumHDonors', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('nHD Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('nHD', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_NumHDonors.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM5vZWanFe3c"
      },
      "source": [
        "**Statistical analysis | Mann-Whitney U Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS-rOqq7Fd1E"
      },
      "outputs": [],
      "source": [
        "mannwhitney('NumHDonors')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOYQ3QiSyu7-"
      },
      "source": [
        "#### **NumHAcceptors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCw6tgNCyxHf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'NumHAcceptors', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('nHA Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('nHA', fontsize=14, fontweight='bold')\n",
        "\n",
        "\n",
        "plt.savefig('plot_NumHAcceptors.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEQoDZctFtGG"
      },
      "outputs": [],
      "source": [
        "mannwhitney('NumHAcceptors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZfRetty7S5n"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'TPSA', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('TPSA Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('TPSA', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_TPSA.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UqXpoEi7cbl"
      },
      "outputs": [],
      "source": [
        "mannwhitney('TPSA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrEziBWM9b7S"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.5, 5.5))\n",
        "\n",
        "sns.boxplot(x = 'bioactivity_class', y = 'NumRotatableBonds', data = df_final, hue='bioactivity_class')\n",
        "\n",
        "plt.xlabel('nRot Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('nRot', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.savefig('plot_TPSA.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkgqoB9n9y1p"
      },
      "outputs": [],
      "source": [
        "mannwhitney('NumRotatableBonds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHca-ttQxlmR"
      },
      "source": [
        "# **BDescriptor Calculation and Dataset Preparation**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abtr2CxPxlmS"
      },
      "source": [
        "## **Download PaDEL-Descriptor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpTI5dKMxlmS"
      },
      "outputs": [],
      "source": [
        "! wget https://github.com/dataprofessor/bioinformatics/raw/master/padel.zip\n",
        "! wget https://github.com/dataprofessor/bioinformatics/raw/master/padel.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64HnTL4tS-nA"
      },
      "outputs": [],
      "source": [
        "! unzip padel.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orb3Xw44xlmT"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic79eppbAA9a"
      },
      "outputs": [],
      "source": [
        "df3 = df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDJkY43R-9F"
      },
      "outputs": [],
      "source": [
        "selection = ['canonical_smiles','molecule_chembl_id']\n",
        "df3_selection = df3[selection]\n",
        "df3_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRSCoPVDSkf5"
      },
      "outputs": [],
      "source": [
        "! cat molecule.smi | head -5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlYaJ9pzUGjS"
      },
      "outputs": [],
      "source": [
        "! cat molecule.smi | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFW3Mk9zxlmT"
      },
      "source": [
        "## **Calculate fingerprint descriptors**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsjnVFMMxlmU"
      },
      "source": [
        "### **Calculate PaDEL descriptors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSCopQvEiSMj"
      },
      "outputs": [],
      "source": [
        "! cat padel.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kN9jrGpS5nE"
      },
      "outputs": [],
      "source": [
        "! bash padel.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p7rAVy_k_hH"
      },
      "outputs": [],
      "source": [
        "! ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxsLzDxKOUrN"
      },
      "outputs": [],
      "source": [
        "df3_X = pd.read_csv('descriptors_output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hHk_H5o6kOW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('descriptors_output.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3kiO0gNxlmU"
      },
      "source": [
        "## **Preparing the X and Y Data Matrices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30aa4WP4ZA8M"
      },
      "source": [
        "### **X data matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acl7m5CtB6Qf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WonFRxAvd28V"
      },
      "outputs": [],
      "source": [
        "df3_X=pd.read_csv('/content/descriptors_output_pIC50.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSyvs11MCB3A"
      },
      "outputs": [],
      "source": [
        "df3_X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_7 = df3_X.drop(columns=['molecule_chembl_id', 'pIC50', 'bioactivity_class', 'canonical_smiles', 'MW', 'LogP', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', 'TPSA'])\n",
        "df_7"
      ],
      "metadata": {
        "id": "sPOE_SZaNxDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXEtORjwJr55"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "if not isinstance(df_7, pd.DataFrame):\n",
        "    df_7 = pd.DataFrame(df_7)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df_7.corr()\n",
        "\n",
        "# Find columns with high correlation\n",
        "high_corr_columns = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.90:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            high_corr_columns.add(colname)\n",
        "\n",
        "# Drop columns with high correlation\n",
        "df_7_filtered = df_7.drop(columns=high_corr_columns)\n",
        "print(f\"Removed columns: {high_corr_columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qQCpX097qf_"
      },
      "source": [
        "### **3.4. Remove low variance features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjHK2SoI7tXI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "selection = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
        "df_7_filtered = selection.fit_transform(df_7_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hlc_5H9fm3Z"
      },
      "outputs": [],
      "source": [
        "X=df_7_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8YkR4HSgIe0"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXMZvkeIgBJ5"
      },
      "outputs": [],
      "source": [
        "Y=df3_X['bioactivity_class']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y=df3_X['pIC50']"
      ],
      "metadata": {
        "id": "SZQns4boZelb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaUTD1Z9xlmV"
      },
      "source": [
        "## **Combining X and Y variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw7MqTMphSwR"
      },
      "source": [
        "## **1. Import libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-zGSqXohSwx"
      },
      "source": [
        "### **3.3. Let's examine the data dimension**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjhOlkOVhSxR"
      },
      "source": [
        "## **4. Data split (80/20 ratio)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Bmg1HWhSxR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48rVT0BPDgRQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "X, Y = shuffle(X, Y, random_state=42)  # Shuffle X and Y together\n",
        "\n",
        "# ... then continue with train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz1o3c1LhSxU"
      },
      "outputs": [],
      "source": [
        "X_train.shape, Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tnwDASChSxW"
      },
      "outputs": [],
      "source": [
        "X_test.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFpLoNRHeRa6"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJRyD1aVOXu3"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGAHlHYSFvmm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GradientBoostingClassifier and GridSearchCV\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(gbc, param_grid=param_grid, cv=10, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_gbc = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPqz2FbbFxEF"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'n_estimators': [300, 500, 700]\n",
        "}\n",
        "\n",
        "# Initialize LGBMClassifier and GridSearchCV\n",
        "lgbm = lgb.LGBMClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(lgbm, param_grid=param_grid, cv=10, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_lgbm = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNsn57zhF3rN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],  # Test different numbers of neighbors\n",
        "    'weights': ['uniform', 'distance'],  # Test different weight functions\n",
        "    'metric': ['euclidean', 'manhattan']  # Test different distance metrics\n",
        "}\n",
        "\n",
        "# Initialize KNeighborsClassifier and GridSearchCV\n",
        "knn = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn, param_grid=param_grid, cv=10, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_knn = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJJD3qKYGQdd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500, 700, 900],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': [3,5,7]\n",
        "}\n",
        "\n",
        "# Initialize ExtraTreesClassifier and GridSearchCV\n",
        "etc = ExtraTreesClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(etc, param_grid=param_grid, cv=10, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_etc = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_etc.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4igmwVEDqHvF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': [3,5,7]\n",
        "}\n",
        "\n",
        "# Initialize DecisionTreeClassifier and GridSearchCV\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(dtc, param_grid=param_grid, cv=10, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_dtc = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_dtc.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC3YhgpdqeXM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np #Added this line to import the numpy module\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'solver': ['liblinear', 'sag', 'saga', 'newton-cg', 'lbfgs'],\n",
        "    'max_iter':[1000]\n",
        "}\n",
        "\n",
        "# Initialize LogisticRegression and GridSearchCV\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "grid_search = GridSearchCV(logreg, param_grid=param_grid, cv=10, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_logreg = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzwQBNArw4fA"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.1, 1, 10]}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "grid_search = GridSearchCV(SVC(kernel='rbf', random_state=42),\n",
        "                           param_grid, cv=10)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "best_svm = grid_search.best_estimator_\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZRoL-0UxRs5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for GaussianProcessClassifier\n",
        "param_grid = {\n",
        "    'kernel': [RBF(length_scale=length_scale)\n",
        "               for length_scale in [0.1, 1.0, 10.0]],\n",
        "    'kernel__length_scale_bounds': [(1e-5, 1e3)]\n",
        "}\n",
        "\n",
        "# Initialize GaussianProcessClassifier and GridSearchCV\n",
        "gpc = GaussianProcessClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(gpc, param_grid, cv=10)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_gpc = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_gpc.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHsI_G8uwyGi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for RandomForestClassifier\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500,700],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': [3,5,7],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Initialize RandomForestClassifier and GridSearchCV\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\"Nearest_Neighbors\", \"SVC\", \"Gaussian_Process\",\"Gradient_Boosting\", \"DecisionTreeClassifier\", \"ExtraTreesClassifier\", \"RandomForestClassifier\", \"LogisticRegression\", \"lightgbm\"]\n",
        "\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(n_neighbors=11, metric='manhattan', weights= 'distance'),\n",
        "    SVC(kernel=\"rbf\", C=1, gamma=0.1, random_state=42),\n",
        "    GaussianProcessClassifier(1.0 * RBF(length_scale=1, length_scale_bounds=(1e-05, 1000.0)), random_state=42),\n",
        "    GradientBoostingClassifier(n_estimators=500 , max_depth=3, min_samples_split=2, learning_rate=0.1, random_state=42),\n",
        "    DecisionTreeClassifier(criterion= 'entropy', max_depth= 15, max_features= 5, min_samples_leaf= 1, min_samples_split= 2),\n",
        "    ExtraTreesClassifier(criterion= 'gini', max_depth= 15, max_features= 7, min_samples_leaf= 2, min_samples_split= 10, n_estimators= 100),\n",
        "    RandomForestClassifier(criterion= 'entropy', max_depth= 15, max_features= 7, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 500),\n",
        "    LogisticRegression(C=0.23357214690901212, max_iter=1000, penalty= 'l2', solver= 'lbfgs'),\n",
        "    lgb.LGBMClassifier(n_estimators=700, learning_rate=0.01, max_depth=5, random_state=42)]"
      ],
      "metadata": {
        "id": "I2zwoSg42rI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h_l_Gqww90h"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import pandas as pd\n",
        "\n",
        "# Evaluate each classifier on Train, CV, and Test sets\n",
        "results = []\n",
        "\n",
        "for name, clf in zip(names, classifiers):\n",
        "    # Training set evaluation\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_train_pred = clf.predict(X_train)\n",
        "\n",
        "    accuracy_train = accuracy_score(Y_train, Y_train_pred)\n",
        "\n",
        "    mcc_train = matthews_corrcoef(Y_train, Y_train_pred)\n",
        "\n",
        "    # Cross-validation\n",
        "    Y_cv_pred = cross_val_predict(clf, X_train, Y_train, cv=10)\n",
        "\n",
        "    accuracy_cv = accuracy_score(Y_train, Y_cv_pred)\n",
        "\n",
        "    mcc_cv = matthews_corrcoef(Y_train, Y_cv_pred)\n",
        "\n",
        "    # Test set evaluation\n",
        "    Y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy_test = accuracy_score(Y_test, Y_test_pred)\n",
        "\n",
        "    mcc_test = matthews_corrcoef(Y_test, Y_test_pred)\n",
        "\n",
        "    results.append({\n",
        "        \"Classifier\": name,\n",
        "        \"Accuracy_Train\": accuracy_train,\n",
        "\n",
        "        \"MCC_Train\": mcc_train,\n",
        "        \"Accuracy_CV\": accuracy_cv,\n",
        "\n",
        "        \"MCC_CV\": mcc_cv,\n",
        "        \"Accuracy_Test\": accuracy_test,\n",
        "\n",
        "        \"MCC_Test\": mcc_test\n",
        "    })\n",
        "\n",
        "# Display the results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Define regressor models and their hyperparameter grids\n",
        "models = {\n",
        "    'Decision Tree': (DecisionTreeRegressor(random_state=42), {'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}),\n",
        "    'Extra Trees': (ExtraTreesRegressor(random_state=42), {'n_estimators': [100, 200, 500], 'max_depth': [None, 5, 10, 20]}),\n",
        "    'Support Vector': (SVR(kernel='rbf', C=1), {'kernel': ['linear', 'rbf', 'poly'], 'C': [0.1, 1, 10]}),\n",
        "    'k-Nearest Neighbors': (KNeighborsRegressor(), {'n_neighbors': [3, 5, 7, 9]}),\n",
        "    'Random Forest': (RandomForestRegressor(random_state=42), {'n_estimators': [100, 200, 500], 'max_depth': [None, 5, 10, 20]}),\n",
        "    'Gaussian Process': (GaussianProcessRegressor(), {'kernel': [1.0 * RBF(1.0)]}),\n",
        "    'Gradient Boosting': (GradientBoostingRegressor(random_state=42), {'n_estimators': [100, 200, 500], 'learning_rate': [0.1, 0.05, 0.01]}),\n",
        "    'LGBM': (lgb.LGBMRegressor(random_state=42), {'n_estimators': [100, 200, 500], 'learning_rate': [0.1, 0.05, 0.01]})\n",
        "\n",
        "}\n",
        "# Perform hyperparameter tuning for each model\n",
        "results = {}\n",
        "for model_name, (model, param_grid) in models.items():\n",
        "    # Use GridSearchCV or RandomizedSearchCV based on your preference\n",
        "    search = GridSearchCV(model, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
        "    search.fit(X_train, Y_train)\n",
        "\n",
        "    best_model = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_rmse = np.sqrt(mean_squared_error(Y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(Y_test, y_test_pred))\n",
        "\n",
        "    train_r2 = r2_score(Y_train, y_train_pred)\n",
        "\n",
        "    test_r2 = r2_score(Y_test, y_test_pred)\n",
        "\n",
        "    # Calculate MAPE\n",
        "    train_mape = mean_absolute_percentage_error(Y_train, y_train_pred)\n",
        "    test_mape = mean_absolute_percentage_error(Y_test, y_test_pred)\n",
        "\n",
        "    # Calculate Q2_CV using cross-validation\n",
        "    cv_scores = cross_val_score(best_model, X_train, Y_train, cv=10, scoring='r2')\n",
        "    Q2_CV = np.mean(cv_scores)\n",
        "\n",
        "    results[model_name] = [best_params, {'train_rmse': train_rmse, 'train_r2': train_r2, 'test_rmse': test_rmse, 'test_r2': test_r2, 'Q2_CV': Q2_CV, 'train_mape': train_mape, 'test_mape': test_mape}]\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "    print(f\"Train R-squared: {train_r2:.4f}\")\n",
        "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "    print(f\"Test R-squared: {test_r2:.4f}\")\n",
        "    print(f\"Q2_CV: {Q2_CV:.4f}\")\n",
        "    print(f\"Train MAPE: {train_mape:.4f}\")\n",
        "    print(f\"Test MAPE: {test_mape:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Compare results\n",
        "best_model = min(results, key=lambda x: results[x]['test_rmse'])\n",
        "print(\"Best Model:\")\n",
        "print(best_model)\n",
        "print(results[best_model])"
      ],
      "metadata": {
        "id": "328zAhw3Z94Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([pd.DataFrame(X_train), Y_train.reset_index(drop=True)], axis=1)\n",
        "test = pd.concat([pd.DataFrame(X_test), Y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "train['model'] = \"Train\"\n",
        "test['model'] = \"Test\""
      ],
      "metadata": {
        "id": "wNDyKwjNB6In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['model'] = \"Train\"\n",
        "test['model'] = \"Test\""
      ],
      "metadata": {
        "id": "bsOgRPN9CEL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [train,test]\n",
        "pca = pd.concat(frames)\n",
        "pca.head(2)"
      ],
      "metadata": {
        "id": "Q9R5si4ECI6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from pandas.plotting import scatter_matrix\n",
        "# Plot the Figures Inline\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def PCA_plot(data):\n",
        "    # PCA's components graphed in 2D\n",
        "    # Apply Scaling\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from matplotlib import pyplot as plt\n",
        "    data_pca = data\n",
        "\n",
        "    # Apply Scaling\n",
        "    X = data_pca.drop('model', axis=1).values\n",
        "    y = data_pca['model'].values\n",
        "\n",
        "    # Formatting\n",
        "    target_names = ['Train','Test']\n",
        "    colors = ['blue','red']\n",
        "\n",
        "    # 2 Components PCA\n",
        "\n",
        "    fig=plt.figure(2, figsize=(7, 5))\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    X_std = StandardScaler().fit_transform(X)\n",
        "    X_r = pca.fit_transform(X_std)\n",
        "\n",
        "    for color, i, target_name in zip(colors, ['Train','Test'], target_names):\n",
        "        plt.scatter(X_r[y == i, 0], X_r[y == i, 1],\n",
        "                    color=color,\n",
        "\n",
        "                    label=target_name)\n",
        "    plt.xlabel('PC1',  fontstyle= \"normal\", fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('PC2',  fontstyle= \"normal\", fontsize=12, fontweight='bold')\n",
        "\n",
        "\n",
        "    plt.grid(False) #remove grid in 2D plot\n",
        "\n",
        "    # Add outline around axes\n",
        "    ax = plt.gca()\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_linewidth(1)\n",
        "        spine.set_color('black')\n",
        "\n",
        "    # Legend with box and black outline\n",
        "    legend = plt.legend(loc='best', scatterpoints=1)\n",
        "    legend.get_frame().set_linewidth(2)\n",
        "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.tick_params(direction=\"out\", labelsize=12)\n",
        "    plt.savefig('applicability_domain.pdf', dpi=1200)\n",
        "PCA_plot(pca)"
      ],
      "metadata": {
        "id": "P0l4qZUOBXQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdPyQYjeg90D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "best_model = LGBMClassifier()\n",
        "best_model.fit(X_train, Y_train)\n",
        "\n",
        "# Applying PCA to training data\n",
        "pca_train = PCA(n_components=2)\n",
        "X_train_pca = pca_train.fit_transform(X_train)\n",
        "\n",
        "# Applying PCA to test data\n",
        "pca_test = PCA(n_components=2)\n",
        "X_test_pca = pca_test.fit_transform(X_test)\n",
        "\n",
        "# Visualize PCA of training data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], color='blue', label='Training-set')\n",
        "\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "# Overlay PCA of test data on the same plot\n",
        "plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], color='red', label='Test-set')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('PCA.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_final=pd.read_csv('/content/MMP13_pIC50_cleaned.csv')"
      ],
      "metadata": {
        "id": "AZlCGAG3wK__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df_final is your DataFrame\n",
        "features = ['MW', 'LogP', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', 'TPSA']\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_final[features]\n",
        "y = df_final['bioactivity_class']\n",
        "\n",
        "# Standardize the data (important for PCA)\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)  # You can change the number of components as needed\n",
        "principal_components = pca.fit_transform(X_std)\n",
        "\n",
        "# Create a new DataFrame with the principal components and group information\n",
        "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "pca_df['Groups'] = y.values\n",
        "\n",
        "# Plot the PCA results\n",
        "plt.figure(figsize=(6.5, 6.5))\n",
        "colors = {'active': 'red', 'inactive': 'blue'}  # Adjust colors as needed\n",
        "for group, color in colors.items():\n",
        "    group_data = pca_df[pca_df['Groups'] == group]\n",
        "    plt.scatter(group_data['PC1'], group_data['PC2'], c=color, label=group, alpha=0.25)\n",
        "\n",
        "\n",
        "plt.xlabel('PC1', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('PC2', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('PCA of active.inactive.pdf')"
      ],
      "metadata": {
        "id": "XycC1LmUjr5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats\n",
        "\n",
        "# Assuming 'df_final' is your DataFrame containing the relevant columns\n",
        "\n",
        "# List of columns\n",
        "columns_of_interest = [\"MW\", \"LogP\", \"NumHAcceptors\", \"NumHDonors\", \"NumRotatableBonds\", \"TPSA\"]\n",
        "\n",
        "# Dictionary to store results\n",
        "summary_statistics = {}\n",
        "\n",
        "# Separate data for group1 and group2\n",
        "for group_name in df_final['bioactivity_class'].unique():\n",
        "    summary_df = df_final[df_final['bioactivity_class'] == group_name][columns_of_interest].describe().transpose()\n",
        "\n",
        "    # Calculate skewness and kurtosis\n",
        "    skewness_values = df_final[df_final['bioactivity_class'] == group_name][columns_of_interest].skew()\n",
        "    kurtosis_values = df_final[df_final['bioactivity_class'] == group_name][columns_of_interest].kurt()\n",
        "\n",
        "    # Add skewness and kurtosis to the summary DataFrame\n",
        "    summary_df['skew'] = skewness_values\n",
        "    summary_df['kurt'] = kurtosis_values\n",
        "\n",
        "    # Add p-values to the table\n",
        "    p_values = []\n",
        "    for column in columns_of_interest:\n",
        "        _, p_value = scipy.stats.ttest_ind(\n",
        "            df_final[df_final['bioactivity_class'] == 'active'][column].dropna(),\n",
        "            df_final[df_final['bioactivity_class'] == 'inactive'][column].dropna()\n",
        "        )\n",
        "        p_values.append(p_value)\n",
        "\n",
        "    # Append p-values to the summary DataFrame\n",
        "    summary_df['p-value'] = p_values\n",
        "\n",
        "    # Store the summary DataFrame in the dictionary\n",
        "    summary_statistics[group_name] = summary_df\n",
        "\n",
        "# Display the results\n",
        "for group_name, stats_df in summary_statistics.items():\n",
        "    print(f\"\\nSummary statistics for {group_name}:\")\n",
        "    print(stats_df[['min', 'max', '50%', 'mean', 'skew', 'kurt', 'p-value']])"
      ],
      "metadata": {
        "id": "sOwVWGhEnZSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}